{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern recognition CW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import file with 6 main components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "camId : which camera was used to get the shot (1 or 2)\n",
    "filelist: names of the images (with format x_label_camId_index.png)\n",
    "labels: class of the image (which person's image is it?)\n",
    "query_idx: indexes of test set\n",
    "gallery_idx: indexes of test set used for kNN \n",
    "train_idx: indexes of training and validation set\n",
    "\"\"\"   \n",
    "train_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import file with the feature vectors of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14096, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('PR_data/feature_data.json', 'r') as f:\n",
    "    features = json.load(f)\n",
    "features_np = np.array(features) #list of features converted to an array \n",
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14096, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5328, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs['gallery_idx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7368, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs['train_idx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12696"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7368+5328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs['query_idx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14096"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12696+1400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classification for query set on gallery set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_filename(image_index): \n",
    "    name = str(train_idxs['filelist'][image_index][0][0])\n",
    "    [dc, label, camId, index] = name.split('_')\n",
    "    return label, camId\n",
    "\n",
    "def delete_images(query_image, query_index, gallery_images, gallery_labels):\n",
    "    gallery_names = train_idxs['filelist'][train_idxs['gallery_idx'].flatten()]\n",
    "    query_label, query_camId = extract_info_from_filename(query_index)\n",
    "    new_gallery_images = []\n",
    "    new_gallery_labels = []\n",
    "    for img, name in zip(gallery_images, gallery_names):\n",
    "        [dc, label_g, camId_g, index] = str(name[0][0]).split('_')\n",
    "        if [int(label_g), int(camId_g)] != [int(query_label), int(query_camId)]:\n",
    "            new_gallery_images.append(img)\n",
    "            new_gallery_labels.append(int(label_g))\n",
    "    return new_gallery_images, new_gallery_labels\n",
    "\n",
    "def score_rank(query_label, rank_k, new_gallery_labels):\n",
    "    rank_k_labels = np.array(new_gallery_labels)[rank_k.flatten()] #extract the labels which are of rank k\n",
    "    return (query_label[0] in rank_k_labels) #return true if query label in rank k, else false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNClassification_deletions(query_image, query_index, gallery_images, gallery_labels, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    new_gallery_images, new_gallery_labels = delete_images(query_image, query_index, gallery_images, gallery_labels)\n",
    "    neigh.fit(new_gallery_images)   #new_gallery_labels)\n",
    "    distances, indices = neigh.kneighbors(query_image, k)\n",
    "    return distances, indices, new_gallery_labels #neigh.score(query_image, query_label), neigh.predict(query_image), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gallery_labels = train_idxs['labels'][train_idxs['gallery_idx'].flatten()]\n",
    "query_labels = train_idxs['labels'][train_idxs['query_idx'].flatten()]\n",
    "\n",
    "query_images = features_np[train_idxs['query_idx'].flatten()]\n",
    "gallery_images = features_np[train_idxs['gallery_idx'].flatten()]\n",
    "\n",
    "def get_accuracy_for_k_ranks(k):\n",
    "    list_of_truths = []\n",
    "    start = time.time() #time tracking - start time of process\n",
    "    for index, query_index in enumerate(train_idxs['query_idx'].flatten().tolist()):\n",
    "        if index == 100:\n",
    "            acc = sum(list_of_truths)/len(list_of_truths)\n",
    "            print(\"Avg accuracy derived for 100 query images is: \", acc)\n",
    "            break \n",
    "        if (index % 10 == 0):\n",
    "            print(\"Index: \", index, \" & time taken: \", time.time() - start)\n",
    "\n",
    "        #perform NN Classification and extract top k ranks \n",
    "        dist, rank_k_indices, new_gallery_labels = NNClassification_deletions(query_images[index].reshape(1, 2048), query_index, gallery_images, gallery_labels, k)\n",
    "        #calculate the score (true/false) for top k images and query image recognition\n",
    "        score = score_rank(query_labels[index], rank_k_indices, new_gallery_labels)\n",
    "        #create a list of scores to get overall accuracy later on\n",
    "        list_of_truths.append(score)\n",
    "    \n",
    "    #over-all accuracy  \n",
    "    acc = sum(list_of_truths)/len(list_of_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  8.916854858398438e-05\n",
      "(array([[14.1799054]]), array([[148]]))\n",
      "(array([[16.31967411]]), array([[1964]]))\n",
      "(array([[14.07498539]]), array([[234]]))\n",
      "(array([[12.77756939]]), array([[9]]))\n",
      "(array([[14.69897922]]), array([[1466]]))\n",
      "(array([[15.56098993]]), array([[1188]]))\n",
      "(array([[17.89340293]]), array([[2492]]))\n",
      "(array([[16.70206892]]), array([[1548]]))\n",
      "(array([[13.39427766]]), array([[32]]))\n",
      "(array([[15.60841844]]), array([[2205]]))\n",
      "Index:  10  & time taken:  7.09958815574646\n",
      "(array([[11.98614757]]), array([[40]]))\n",
      "(array([[11.57795572]]), array([[39]]))\n",
      "(array([[10.84431517]]), array([[1138]]))\n",
      "(array([[15.9575077]]), array([[45]]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e4bfa041689a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_accuracy_for_k_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-1d29d73ea708>\u001b[0m in \u001b[0;36mget_accuracy_for_k_ranks\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#perform NN Classification and extract top k ranks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNNClassification_deletions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#dist, rank_k_indices, new_gallery_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#calculate the score (true/false) for top k images and query image recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-82efc31acedc>\u001b[0m in \u001b[0;36mNNClassification_deletions\u001b[0;34m(query_image, query_index, gallery_images, gallery_labels, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_gallery_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_gallery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelete_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_gallery_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_gallery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m   \u001b[0;31m#new_gallery_labels #neigh.score(query_image, query_label), neigh.predict(query_image),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \"\"\"\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    246\u001b[0m             self._tree = KDTree(X, self.leaf_size,\n\u001b[1;32m    247\u001b[0m                                 \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                                 **self.effective_metric_params_)\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'brute'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_accuracy_for_k_ranks(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  6.198883056640625e-05\n",
      "Index:  10  & time taken:  7.4820661544799805\n",
      "Index:  20  & time taken:  16.403679132461548\n",
      "Index:  30  & time taken:  24.638787269592285\n",
      "Index:  40  & time taken:  32.07371497154236\n",
      "Index:  50  & time taken:  40.546473026275635\n",
      "Index:  60  & time taken:  49.52488303184509\n",
      "Index:  70  & time taken:  57.03961133956909\n",
      "Index:  80  & time taken:  65.4046561717987\n",
      "Index:  90  & time taken:  73.00216317176819\n",
      "Avg accuracy derived for 100 query images is:  0.69\n",
      "0.69\n"
     ]
    }
   ],
   "source": [
    "get_accuracy_for_k_ranks(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  8.988380432128906e-05\n",
      "Index:  10  & time taken:  7.302855968475342\n",
      "Index:  20  & time taken:  14.723245859146118\n",
      "Index:  30  & time taken:  22.034220695495605\n",
      "Index:  40  & time taken:  28.93511986732483\n",
      "Index:  50  & time taken:  35.75521993637085\n",
      "Index:  60  & time taken:  42.58328080177307\n",
      "Index:  70  & time taken:  49.41076993942261\n",
      "Index:  80  & time taken:  56.251181840896606\n",
      "Index:  90  & time taken:  63.07164788246155\n",
      "Avg accuracy derived for 100 query images is:  0.75\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "get_accuracy_for_k_ranks(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
