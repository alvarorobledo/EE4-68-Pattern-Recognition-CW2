{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern recognition CW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import metric_learn\n",
    "from numpy import linalg as calc_eigen\n",
    "from sklearn.preprocessing import normalize as normalize_vectors\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import file with 6 main components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "camId : which camera was used to get the shot (1 or 2)\n",
    "filelist: names of the images (with format x_label_camId_index.png)\n",
    "labels: class of the image (which person's image is it?)\n",
    "query_idx: indexes of test set\n",
    "gallery_idx: indexes of test set used for kNN \n",
    "train_idx: indexes of training and validation set\n",
    "\"\"\"   \n",
    "train_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import file with the feature vectors of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14096, 2048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('PR_data/feature_data.json', 'r') as f:\n",
    "    features = json.load(f)\n",
    "features_np = np.array(features) #list of features converted to an array \n",
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_indices = train_idxs['gallery_idx'].flatten() - 1\n",
    "gallery_images = features_np[gallery_indices]\n",
    "gallery_labels = train_idxs['labels'][gallery_indices]\n",
    "\n",
    "query_indices = train_idxs['query_idx'].flatten() - 1\n",
    "query_images = features_np[query_indices]\n",
    "query_labels = train_idxs['labels'][query_indices]\n",
    "\n",
    "training_indices = train_idxs['train_idx'].flatten() - 1\n",
    "training_images = features_np[training_indices]\n",
    "training_labels = train_idxs['labels'][training_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAverageFace(X):\n",
    "    \"\"\"\n",
    "    takes the training data set as input \n",
    "    and returns the average face\n",
    "    \"\"\"\n",
    "    average_face= np.mean(X, axis=0)\n",
    "    #plt.figure()\n",
    "    #plt.title(\"Average Face\")\n",
    "    #plt.imshow(np.resize(average_face, (46, 56)).T, cmap='gray')\n",
    "    return average_face\n",
    "\n",
    "def normalize(X, AvgFace):\n",
    "    \"\"\"\n",
    "    takes the training data set and average face as input \n",
    "    and returns the normalized training data set\n",
    "\n",
    "    \"\"\"\n",
    "    Q = np.empty((len(X[:,0]),len(X[0, :])))\n",
    "    for index, face in enumerate(X):\n",
    "        Q[index] = face - AvgFace\n",
    "    return Q.T\n",
    "\n",
    "def calculateCovarianceMatrix(Q):\n",
    "    return np.matmul(Q,Q.T)/len(Q[0,:])\n",
    "\n",
    "def calculateLowDimCovarianceMatrix(Q):\n",
    "    return np.matmul(Q.T,Q)/len(Q[0,:])\n",
    "\n",
    "def calculateEigenValuesAndVectors(S):\n",
    "    \"\"\"\n",
    "    Takes covariance matrix as input\n",
    "    and returns eigen values and vectors\n",
    "    \"\"\"\n",
    "    v, w =  calc_eigen.eigh(S)\n",
    "    #eigen_vectors[:,i] --> eigen_values[i]\n",
    "    #eigen vector corresponding to eigen value\n",
    "\n",
    "    #flips left to right... ascending to descending\n",
    "    v = np.flip(v, axis=0) #turn ascending into descending\n",
    "    w = np.flip(w, axis=1) #turn ascending into descending\n",
    "    return v, w\n",
    "\n",
    "def calculateWeights(Q, U):\n",
    "    \"\"\"\n",
    "    takes input the normalized input and the eigen space\n",
    "    outputs the weights of the normalized input\n",
    "    \"\"\"\n",
    "    N = len(Q[0,:])\n",
    "    W = np.empty((N,len(U[0, :])))\n",
    "    for index, image in enumerate(Q.T):\n",
    "        W[index] = np.matmul(image, U)\n",
    "    return W\n",
    "\n",
    "def calculateWeights2(Q, U):\n",
    "    \"\"\"\n",
    "    alternative method to calculate weights\n",
    "    \"\"\"\n",
    "    N = len(Q[0,:])\n",
    "    W2 = np.empty((N,len(U[0, :])))\n",
    "    W2 = np.matmul(Q.T, top_eigen_vectors)\n",
    "    return W2\n",
    "\n",
    "def printImage(face, title, saved_file):\n",
    "    \"\"\"\n",
    "    takes input as the face you want to print, the title of the image, and the location of the file\n",
    "    you would like to save the image to. \n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(np.resize(face, (46, 56)).T, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(saved_file, bbox_inches='tight')   # save the figure to file\n",
    "    plt.show()\n",
    "    plt.close() \n",
    "    \n",
    "def plotEigenValueGraph(v, points):\n",
    "    \"\"\"\n",
    "    plots eigen values against inrementing the number of eigen values used\n",
    "    in descending order\n",
    "    \"\"\"\n",
    "    y_points = [value for value in v[:points]]\n",
    "    x_points = [i for i in range(points)]\n",
    "    plt.xlabel('Number of Eigen Values')\n",
    "    plt.ylabel('Eigen Values')\n",
    "    plt.title('Plot of Eigen Values in Descending Order')\n",
    "    plt.plot(x_points, y_points)\n",
    "    plt.savefig('eigenvalues.png', bbox_inches='tight')   # save the figure to file\n",
    "    plt.show()\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Training, Gallery, and Query Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, M):\n",
    "    AvgFace = calculateAverageFace(X)\n",
    "    A = normalize(X, AvgFace)\n",
    "    S = calculateCovarianceMatrix(A)\n",
    "    eigen_values, eigen_vectors = calculateEigenValuesAndVectors(S)\n",
    "    x_points = 1000\n",
    "    #plotEigenValueGraph(eigen_values, x_points)\n",
    "    variance_captured = np.sum(eigen_values[:M])*100/np.sum(eigen_values)\n",
    "    print(\"Variance Captured by top \", x_points, \" features is \", variance_captured, \".\")\n",
    "    top_eigen_vectors = eigen_vectors[:, :M]\n",
    "    reduced_dim_X =  calculateWeights(A, top_eigen_vectors)\n",
    "    return reduced_dim_X, top_eigen_vectors, AvgFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Captured by top  1000  features is  92.92050565369442 .\n"
     ]
    }
   ],
   "source": [
    "M = 100\n",
    "reduced_training_images, eigen_space, AvgFace = PCA(training_images, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_gallery_images = calculateWeights(normalize(gallery_images, AvgFace), eigen_space)\n",
    "reduced_query_images = calculateWeights(normalize(query_images, AvgFace), eigen_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classification for query set on gallery set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_index(value, recall): #only used in calcAP function below\n",
    "    for index, recall_val in enumerate(recall):\n",
    "        if recall_val >= value:\n",
    "            return index\n",
    "    \n",
    "def calcAP(true_label, rank_k_labels):\n",
    "    correct_list = [true_label == this_label for this_label in rank_k_labels]    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(rank_k_labels)):\n",
    "        precision.append(sum(correct_list[:i+1])/len(correct_list[:i+1]))\n",
    "        recall.append(sum(correct_list[:i+1])/sum(correct_list))\n",
    "    max_list = []\n",
    "    for i in range(11):\n",
    "        recall_index = calculate_recall_index(i/10, recall)\n",
    "        max_list.append(max(precision[recall_index:]))\n",
    "    #print(max_list)\n",
    "    return np.mean(max_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indices(n_indices, query_index):\n",
    "    query_label_and_camId = (train_idxs['labels'][query_index][0], train_idxs['camId'][query_index][0])\n",
    "    list_of_gallery_label_and_camId = [(label[0], camId[0]) for label, camId in zip(train_idxs['labels'][n_indices], train_idxs['camId'][n_indices])] \n",
    "    final_list = list(filter(lambda a: a != query_label_and_camId, list_of_gallery_label_and_camId))\n",
    "    return [tup[0] for tup in final_list]\n",
    "\n",
    "def get_accuracy(k, N):\n",
    "    list_of_truths = []\n",
    "    AP_vals = []\n",
    "    start = time.time() #time tracking - start time of process\n",
    "    neigh = NearestNeighbors(n_neighbors=N, n_jobs=-1)\n",
    "    neigh.fit(gallery_images)   #new_gallery_labels)\n",
    "    for index, query_index in enumerate(query_indices.tolist()):\n",
    "        if (index % 100 == 0):\n",
    "            print(\"Index: \", index, \" & time taken: \", time.time() - start)\n",
    "        N_distances, N_indices = neigh.kneighbors(query_images[index].reshape(1, 2048))\n",
    "        forLoopStart = time.time()\n",
    "        topN_gallery_indices = (gallery_indices[N_indices[0]]).tolist()\n",
    "        reduced_topN_labels = remove_indices(topN_gallery_indices, query_index)\n",
    "        if query_labels[index][0] in reduced_topN_labels[:k]:\n",
    "            list_of_truths.append(True)\n",
    "        else:\n",
    "            list_of_truths.append(False)\n",
    "        AP = calcAP(query_labels[index][0], reduced_topN_labels[:k])\n",
    "        AP_vals.append(AP)\n",
    "    #over-all accuracy  \n",
    "    acc = sum(list_of_truths)/len(list_of_truths)\n",
    "    #print(acc)\n",
    "    return acc, np.mean(AP_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank  1 : \n",
      "Index:  0  & time taken:  0.6330201625823975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  100  & time taken:  11.09302806854248\n",
      "Index:  200  & time taken:  21.656609058380127\n",
      "Index:  300  & time taken:  32.17634606361389\n",
      "Index:  400  & time taken:  42.74071907997131\n",
      "Index:  500  & time taken:  53.18624424934387\n",
      "Index:  600  & time taken:  63.734869956970215\n",
      "Index:  700  & time taken:  74.28422927856445\n",
      "Index:  800  & time taken:  84.80377697944641\n",
      "Index:  900  & time taken:  95.39508509635925\n",
      "Index:  1000  & time taken:  106.03644609451294\n",
      "Index:  1100  & time taken:  116.58560514450073\n",
      "Index:  1200  & time taken:  127.08568906784058\n",
      "Index:  1300  & time taken:  137.68114113807678\n",
      "(1, 0.47, 0.47)\n",
      "For rank  5 : \n",
      "Index:  0  & time taken:  0.6057331562042236\n",
      "Index:  100  & time taken:  11.174184083938599\n",
      "Index:  200  & time taken:  21.870043992996216\n",
      "Index:  300  & time taken:  32.56023406982422\n",
      "Index:  400  & time taken:  43.104028940200806\n",
      "Index:  500  & time taken:  53.7527060508728\n",
      "Index:  600  & time taken:  64.34500098228455\n",
      "Index:  700  & time taken:  74.97934126853943\n",
      "Index:  800  & time taken:  85.54107403755188\n",
      "Index:  900  & time taken:  96.17309904098511\n",
      "Index:  1000  & time taken:  106.78644895553589\n",
      "Index:  1100  & time taken:  117.42659306526184\n",
      "Index:  1200  & time taken:  128.05143213272095\n",
      "Index:  1300  & time taken:  138.71504712104797\n",
      "(5, 0.6685714285714286, 0.5272045454545454)\n",
      "For rank  10 : \n",
      "Index:  0  & time taken:  0.6040680408477783\n",
      "Index:  100  & time taken:  11.279120922088623\n",
      "Index:  200  & time taken:  21.997786045074463\n",
      "Index:  300  & time taken:  32.6515588760376\n",
      "Index:  400  & time taken:  43.34866523742676\n",
      "Index:  500  & time taken:  53.998563051223755\n",
      "Index:  600  & time taken:  64.59102010726929\n",
      "Index:  700  & time taken:  75.2099940776825\n",
      "Index:  800  & time taken:  85.9669680595398\n",
      "Index:  900  & time taken:  96.64234399795532\n",
      "Index:  1000  & time taken:  107.31123781204224\n",
      "Index:  1100  & time taken:  118.07735085487366\n",
      "Index:  1200  & time taken:  128.7832808494568\n",
      "Index:  1300  & time taken:  139.54148292541504\n",
      "(10, 0.7492857142857143, 0.5169191146155432)\n"
     ]
    }
   ],
   "source": [
    "N=100\n",
    "k = [1, 5, 10]\n",
    "results = []\n",
    "for k_val in k: \n",
    "    print(\"For rank \", k_val, \": \")\n",
    "    accuracy, mAP = get_accuracy(k_val, N)\n",
    "    tup = (k_val, accuracy, mAP)\n",
    "    results.append(tup)\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for kNN: \n",
    "\n",
    "k = 1, Accuracy = 0.47, mAP = 0.47\n",
    "\n",
    "k = 5, Accuracy = 0.6685714285714286, mAP = 0.5272045454545454\n",
    "\n",
    "k = 10, Accuracy = 0.7492857142857143, mAP = 0.5169191146155432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_pca(k, N):\n",
    "    list_of_truths = []\n",
    "    AP_vals = []\n",
    "    start = time.time() #time tracking - start time of process\n",
    "    neigh = NearestNeighbors(n_neighbors=N, n_jobs=-1)\n",
    "    neigh.fit(reduced_gallery_images)   #new_gallery_labels)\n",
    "    for index, query_index in enumerate(query_indices.tolist()):\n",
    "        if (index % 200 == 0):\n",
    "            print(\"Index: \", index, \" & time taken: \", time.time() - start)\n",
    "        N_distances, N_indices = neigh.kneighbors(reduced_query_images[index].reshape(1, len(reduced_query_images[0, :])))\n",
    "        forLoopStart = time.time()\n",
    "        topN_gallery_indices = (gallery_indices[N_indices[0]]).tolist()\n",
    "        reduced_topN_labels = remove_indices(topN_gallery_indices, query_index)\n",
    "        if query_labels[index][0] in reduced_topN_labels[:k]:\n",
    "            list_of_truths.append(True)\n",
    "        else:\n",
    "            list_of_truths.append(False)\n",
    "        AP = calcAP(query_labels[index][0], reduced_topN_labels[:k])\n",
    "        AP_vals.append(AP)\n",
    "    #over-all accuracy  \n",
    "    acc = sum(list_of_truths)/len(list_of_truths)\n",
    "    print(acc)\n",
    "    return acc, np.mean(AP_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank  1 : \n",
      "Index:  0  & time taken:  0.02941107749938965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  200  & time taken:  21.079618215560913\n",
      "Index:  400  & time taken:  42.17799711227417\n",
      "Index:  600  & time taken:  63.17202019691467\n",
      "Index:  800  & time taken:  84.27191829681396\n",
      "Index:  1000  & time taken:  105.21672511100769\n",
      "Index:  1200  & time taken:  126.23884606361389\n",
      "0.46714285714285714\n",
      "(1, 0.46714285714285714, 0.46714285714285714)\n",
      "For rank  5 : \n",
      "Index:  0  & time taken:  0.013506174087524414\n",
      "Index:  200  & time taken:  20.97689700126648\n",
      "Index:  400  & time taken:  41.993072271347046\n",
      "Index:  600  & time taken:  63.114259004592896\n",
      "Index:  800  & time taken:  84.1421959400177\n",
      "Index:  1000  & time taken:  105.16470098495483\n",
      "Index:  1200  & time taken:  126.25219821929932\n",
      "0.6757142857142857\n",
      "(5, 0.6757142857142857, 0.5285400432900433)\n",
      "For rank  10 : \n",
      "Index:  0  & time taken:  0.013849973678588867\n",
      "Index:  200  & time taken:  21.23994207382202\n",
      "Index:  400  & time taken:  42.65201115608215\n",
      "Index:  600  & time taken:  63.94959092140198\n",
      "Index:  800  & time taken:  85.24179100990295\n",
      "Index:  1000  & time taken:  106.2984139919281\n",
      "Index:  1200  & time taken:  127.38018703460693\n",
      "0.7485714285714286\n",
      "(10, 0.7485714285714286, 0.5152942434549578)\n"
     ]
    }
   ],
   "source": [
    "N=100\n",
    "k = [1, 5, 10]\n",
    "results2 = []\n",
    "for k_val in k: \n",
    "    print(\"For rank \", k_val, \": \")\n",
    "    accuracy, mAP = get_accuracy_pca(k_val, N)\n",
    "    tup = (k_val, accuracy, mAP)\n",
    "    results2.append(tup)\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for kNN using PCA: \n",
    "\n",
    "k = 1, Accuracy = 0.46714285714285714, mAP = 0.46714285714285714\n",
    "\n",
    "k = 5, Accuracy = 0.6757142857142857, mAP = 0.5285400432900433\n",
    "\n",
    "k = 10, Accuracy = 0.7485714285714286, mAP = 0.5152942434549578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_indices_in_cluster(c, index_and_cluster): \n",
    "    gallery_indices_in_cluster = [pair[0] for pair in index_and_cluster if pair[1] == c]\n",
    "    return gallery_indices_in_cluster\n",
    "\n",
    "def img_cluster(k, N, c_g_indices, query_index, index):\n",
    "    list_of_truths = []\n",
    "    neigh = NearestNeighbors(n_neighbors=N, n_jobs=-1, metric='euclidean')\n",
    "    neigh.fit(features_np[c_g_indices])   #new_gallery_labels)\n",
    "    N_distances, N_indices = neigh.kneighbors(query_images[index].reshape(1, -1))\n",
    "    topN_c_g_indices = (gallery_indices[N_indices[0]]).tolist()\n",
    "    reduced_topN_labels = remove_indices(topN_c_g_indices, query_index)\n",
    "    return (query_labels[index][0] in reduced_topN_labels[:k])\n",
    "\n",
    "def knn_clustering(k, N, index_and_cluster):\n",
    "    class_success = []\n",
    "    start = time.time() #time tracking - start time of process\n",
    "    for index, query_index in enumerate(query_indices.tolist()[:700]):\n",
    "        if (index % 200 == 0):\n",
    "            print(\"Index: \", index, \" & time taken: \", time.time() - start)\n",
    "        cluster = kmeans.predict(query_images[index].reshape(1,-1))\n",
    "        cluster_gallery_indices = img_indices_in_cluster(cluster, index_and_cluster)\n",
    "        class_success.append(img_cluster(k, N, cluster_gallery_indices, query_index, index))\n",
    "    \n",
    "    #over-all accuracy\n",
    "    acc = sum(class_success)/len(class_success)\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  4.506111145019531e-05\n",
      "Index:  200  & time taken:  150.7624750137329\n",
      "Index:  400  & time taken:  301.3440308570862\n",
      "Index:  600  & time taken:  461.696977853775\n",
      "0.49714285714285716\n",
      "Index:  0  & time taken:  4.696846008300781e-05\n",
      "Index:  200  & time taken:  156.14981603622437\n",
      "Index:  400  & time taken:  307.5661029815674\n",
      "Index:  600  & time taken:  461.1602611541748\n",
      "0.69\n",
      "Index:  0  & time taken:  5.0067901611328125e-05\n",
      "Index:  200  & time taken:  153.74979496002197\n",
      "Index:  400  & time taken:  308.7922320365906\n",
      "Index:  600  & time taken:  465.1946897506714\n",
      "0.7742857142857142\n",
      "Index:  0  & time taken:  4.792213439941406e-05\n",
      "Index:  200  & time taken:  51.17026996612549\n",
      "Index:  400  & time taken:  104.5521879196167\n",
      "Index:  600  & time taken:  155.86516094207764\n",
      "0.0\n",
      "Index:  0  & time taken:  7.796287536621094e-05\n",
      "Index:  200  & time taken:  51.24071407318115\n",
      "Index:  400  & time taken:  104.89248394966125\n",
      "Index:  600  & time taken:  156.0232548713684\n",
      "0.0014285714285714286\n",
      "Index:  0  & time taken:  7.200241088867188e-05\n",
      "Index:  200  & time taken:  51.43580603599548\n",
      "Index:  400  & time taken:  104.93968510627747\n",
      "Index:  600  & time taken:  156.0580611228943\n",
      "0.005714285714285714\n",
      "Index:  0  & time taken:  4.291534423828125e-05\n",
      "Index:  200  & time taken:  29.55681800842285\n",
      "Index:  400  & time taken:  58.99013090133667\n",
      "Index:  600  & time taken:  87.96334171295166\n",
      "0.0\n",
      "Index:  0  & time taken:  6.914138793945312e-05\n",
      "Index:  200  & time taken:  29.59423804283142\n",
      "Index:  400  & time taken:  59.0254921913147\n",
      "Index:  600  & time taken:  87.95643329620361\n",
      "0.004285714285714286\n",
      "Index:  0  & time taken:  9.584426879882812e-05\n",
      "Index:  200  & time taken:  29.54171586036682\n",
      "Index:  400  & time taken:  58.96396994590759\n",
      "Index:  600  & time taken:  87.91403484344482\n",
      "0.011428571428571429\n"
     ]
    }
   ],
   "source": [
    "total_number_of_labels = len(np.unique(gallery_labels.flatten()))\n",
    "\n",
    "acc_kmeans = []\n",
    "clusters = [1, 3, 10] \n",
    "clusters2 = [100, total_number_of_labels]\n",
    "for n_clusters in clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(gallery_images)\n",
    "    index_and_cluster = [(index, label) for index, label in zip(gallery_indices, kmeans.labels_)]\n",
    "    k = [1, 5, 10]\n",
    "    N = 100\n",
    "    for k_val in k:\n",
    "        acc_kmeans.append(knn_clustering(k_val, N, index_and_cluster)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  5.2928924560546875e-05\n",
      "Index:  200  & time taken:  31.98020100593567\n",
      "Index:  400  & time taken:  60.46903991699219\n",
      "Index:  600  & time taken:  91.8608329296112\n",
      "0.0\n",
      "Index:  0  & time taken:  0.00010395050048828125\n",
      "Index:  200  & time taken:  31.05888295173645\n",
      "Index:  400  & time taken:  58.93773698806763\n",
      "Index:  600  & time taken:  90.08086109161377\n",
      "0.002857142857142857\n",
      "Index:  0  & time taken:  0.00021696090698242188\n",
      "Index:  200  & time taken:  31.06084704399109\n",
      "Index:  400  & time taken:  59.05705428123474\n",
      "Index:  600  & time taken:  89.93986010551453\n",
      "0.005714285714285714\n",
      "Index:  0  & time taken:  6.794929504394531e-05\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-5c181313d687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0macc_kmeans2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_and_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-157-5e27242e27ca>\u001b[0m in \u001b[0;36mknn_clustering\u001b[0;34m(k, N, index_and_cluster)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcluster_gallery_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_indices_in_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_and_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mclass_success\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_gallery_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#over-all accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-157-5e27242e27ca>\u001b[0m in \u001b[0;36mimg_cluster\u001b[0;34m(k, N, c_g_indices, query_index, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_g_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#new_gallery_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mN_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtopN_c_g_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgallery_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreduced_topN_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopN_c_g_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             )\n\u001b[1;32m    349\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 20"
     ]
    }
   ],
   "source": [
    "total_number_of_labels = len(np.unique(gallery_labels.flatten()))\n",
    "acc_kmeans2 = []\n",
    "clusters = [100, total_number_of_labels]\n",
    "for n_clusters in clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(gallery_images)\n",
    "    index_and_cluster = [(index, label) for index, label in zip(gallery_indices, kmeans.labels_)]\n",
    "    k = [1, 5, 10]\n",
    "    N = 20\n",
    "    for k_val in k:\n",
    "        acc_kmeans2.append(knn_clustering(k_val, N, index_and_cluster)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atttempt 1: Mahalanobis Distance Using metric_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = np.cov(np.transpose(training_images))\n",
    "#A = np.linalg.inv(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_maha(k, N):   \n",
    "    start = time.time() #time tracking - start time of process\n",
    "    lmnn = metric_learn.LMNN(k=k)\n",
    "    # fit the data!\n",
    "    lmnn.fit(reduced_training_images, training_labels) #reduced_dim_training_images\n",
    "    # transform our input space\n",
    "    A = lmnn.metric()\n",
    "    S = np.linalg.inv(A)\n",
    "\n",
    "    list_of_truths = []\n",
    "    i = 0\n",
    "    AP_vals = []\n",
    "    for q_index, q_image in zip(query_indices, reduced_query_images): \n",
    "        if (i % 100 == 0):\n",
    "            print(\"Index: \", i, \" & time taken: \", time.time() - start)\n",
    "        query_distances = []\n",
    "        for g_index, g_image in zip(gallery_indices, reduced_gallery_images): \n",
    "            image_diff = q_image - g_image\n",
    "            query_distances.append(tuple([g_index, np.linalg.multi_dot([np.transpose(image_diff), S, image_diff])]))\n",
    "        query_distances_sorted = sorted(query_distances,key=lambda x:(x[1]))\n",
    "        N_gallery_indices = [tup[0] for tup in query_distances_sorted][:N]\n",
    "        reduced_N_gallery_labels = remove_indices(N_gallery_indices, q_index)\n",
    "        if query_labels[i][0] in reduced_N_gallery_labels[:k]:\n",
    "            list_of_truths.append(True)\n",
    "        else:\n",
    "            list_of_truths.append(False)\n",
    "        AP = calcAP(query_labels[i][0], reduced_N_gallery_labels[:k])\n",
    "        AP_vals.append(AP)\n",
    "        i +=1\n",
    "    acc = sum(list_of_truths)/len(list_of_truths) \n",
    "    return acc, np.mean(AP_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank  1 : \n",
      "Index:  0  & time taken:  5.427803039550781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  100  & time taken:  13.546617984771729\n",
      "Index:  200  & time taken:  21.883944988250732\n",
      "Index:  300  & time taken:  29.981092929840088\n",
      "Index:  400  & time taken:  38.06189799308777\n",
      "Index:  500  & time taken:  46.22734594345093\n",
      "Index:  600  & time taken:  54.30570983886719\n",
      "Index:  700  & time taken:  62.39797401428223\n",
      "Index:  800  & time taken:  70.60209202766418\n",
      "Index:  900  & time taken:  78.81504797935486\n",
      "Index:  1000  & time taken:  86.92118906974792\n",
      "Index:  1100  & time taken:  96.24939298629761\n",
      "Index:  1200  & time taken:  104.35860800743103\n",
      "Index:  1300  & time taken:  112.7339038848877\n",
      "(1, 0.13, 0.13)\n",
      "For rank  5 : \n",
      "Index:  0  & time taken:  5.489860773086548\n",
      "Index:  100  & time taken:  13.865935802459717\n",
      "Index:  200  & time taken:  22.857558965682983\n",
      "Index:  300  & time taken:  32.80487608909607\n",
      "Index:  400  & time taken:  40.977351903915405\n",
      "Index:  500  & time taken:  49.45302486419678\n",
      "Index:  600  & time taken:  57.52837085723877\n",
      "Index:  700  & time taken:  66.459969997406\n",
      "Index:  800  & time taken:  75.50546193122864\n",
      "Index:  900  & time taken:  86.02947783470154\n",
      "Index:  1000  & time taken:  94.14292407035828\n",
      "Index:  1100  & time taken:  102.21371293067932\n",
      "Index:  1200  & time taken:  110.26887702941895\n",
      "Index:  1300  & time taken:  118.3265528678894\n",
      "(5, 0.2692857142857143, 0.19095995670995672)\n"
     ]
    }
   ],
   "source": [
    "k = [1, 5]\n",
    "N = 50\n",
    "results3 = []\n",
    "for k_val in k: \n",
    "    print(\"For rank \", k_val, \": \")\n",
    "    accuracy, mAP = knn_maha(k_val, N)\n",
    "    tup = (k_val, accuracy, mAP)\n",
    "    results3.append(tup)\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: Mahalanobis Distance Using Cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.cov(np.transpose(reduced_training_images))\n",
    "S = np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.59143173e-17, -5.26842904e-19,  5.30399860e-17,  3.93850271e-19,\n",
       "       -1.99653840e-18,  2.90425986e-18,  3.60573004e-17, -3.58023330e-17,\n",
       "       -2.68850768e-17,  4.33028843e-17,  4.90452495e-17, -8.25971584e-18,\n",
       "        2.93800106e-17, -6.23003251e-17,  1.74532655e-17, -9.82029995e-17,\n",
       "        1.03953919e-16,  1.61497025e-17,  8.28539364e-17,  2.46735852e-17,\n",
       "        1.09153047e-16,  3.31786616e-17, -6.10328075e-17, -1.11164710e-17,\n",
       "        2.10812941e-17, -6.10108012e-17, -1.67729673e-17,  6.04618801e-17,\n",
       "        4.67698463e-18,  1.55277336e-16, -1.45465364e-16,  5.10472924e-18,\n",
       "        3.10928011e-16, -4.08300533e-17,  2.98490139e-16, -1.71100616e-16,\n",
       "        4.12243474e-16, -8.73094183e-17,  2.68815666e-16, -2.87291729e-17,\n",
       "       -1.72161936e-17, -2.15361594e-16, -1.39882657e-16,  1.22915095e-16,\n",
       "       -1.85143611e-16, -1.80502757e-16, -8.02014801e-17, -3.69328462e-16,\n",
       "       -3.09615432e-17,  3.70857649e-16,  2.68027205e-16, -3.61503404e-16,\n",
       "       -5.29037920e-16, -6.69849840e-16,  2.46848444e-16,  4.27714764e-16,\n",
       "        6.10765844e-16, -1.15564189e-15,  1.60593662e-17, -3.33962548e-16,\n",
       "       -2.14000608e-16,  3.80067284e-16, -5.44109528e-15,  1.18167927e-16,\n",
       "        8.20328103e-17, -1.73969716e-16,  9.12522699e-17, -1.25069514e-16,\n",
       "       -1.83945058e-16,  1.23256738e-16, -4.32929081e-16,  1.08688799e-15,\n",
       "       -3.44519231e-16, -6.26631600e-16,  1.95362402e-16, -1.21322316e-16,\n",
       "       -2.02816298e-16, -3.03476613e-16,  8.21852754e-16,  2.53133125e-16,\n",
       "        1.00163384e-16, -4.11042277e-16,  7.60990573e-16,  5.64638996e-16,\n",
       "        5.50042719e-18,  3.57087680e-16,  6.41786223e-16, -1.17756359e-16,\n",
       "       -1.22956991e-15,  1.26847466e-16, -4.28863415e-16,  1.28883477e-16,\n",
       "        5.91762183e-16,  3.56042357e-16, -9.43090646e-16,  3.42086619e-16,\n",
       "        2.95973900e-16, -2.72776069e-16,  1.16634081e-16,  2.79302961e+00])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[-1] #all features after PCA are independant \n",
    "#covariance matrix A/S gives us 0 on everything except of the diagonal \n",
    "#mahalanobis distance is only useful when features are dependant (why?). whereas, in our case, they are independant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5328)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix = cdist(reduced_query_images, reduced_gallery_images, 'mahalanobis', VI=S)\n",
    "distance_matrix.shape #rows represent query images, columns represent gallery images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gallery_indices_to_distance_matrix(distance_matrix):\n",
    "    new_array = np.zeros((5328,1400),dtype=object)  \n",
    "    distance_matrix2 = []\n",
    "    for g_index, g_row in enumerate(distance_matrix.T): \n",
    "        for q_index, val in enumerate(g_row):\n",
    "            tup1 = tuple([gallery_indices[g_index], val])\n",
    "            new_array[g_index][q_index] = tup1\n",
    "    return new_array\n",
    "\n",
    "def calculate_accuracy_from_distance_matrix(new_array, k, N):\n",
    "    list_of_truths = []\n",
    "    for q_index, column in enumerate(new_array.T):\n",
    "        query_distances_sorted = sorted(column,key=lambda x:(x[1]))\n",
    "        N_gallery_indices = [tup[0] for tup in query_distances_sorted][:N]\n",
    "        reduced_N_gallery_labels = remove_indices(N_gallery_indices, query_indices[q_index])\n",
    "        if query_labels[q_index][0] in reduced_N_gallery_labels[:k]:\n",
    "            list_of_truths.append(True)\n",
    "        else:\n",
    "            list_of_truths.append(False)\n",
    "    acc = sum(list_of_truths)/len(list_of_truths) \n",
    "    return acc\n",
    "\n",
    "def calc_acc_maha(k, N, distance_matrix): \n",
    "    arr = add_gallery_indices_to_distance_matrix(distance_matrix)\n",
    "    acc = calculate_accuracy_from_distance_matrix(arr, k, N)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41785714285714287\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "N = 50\n",
    "print(calc_acc_maha(k, N, distance_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6221428571428571\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "N = 50\n",
    "print(calc_acc_maha(k, N, distance_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7028571428571428\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "N = 50\n",
    "print(calc_acc_maha(k, N, distance_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3: Getting distances using kNN and Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='mahalanobis',\n",
       "         metric_params={'V': array([[ 0.12217,  0.00295, ..., -0.00943, -0.01215],\n",
       "       [ 0.00295,  0.23106, ...,  0.00179, -0.01196],\n",
       "       ...,\n",
       "       [-0.00943,  0.00179, ...,  0.32026,  0.09041],\n",
       "       [-0.01215, -0.01196, ...,  0.09041,  0.15056]])},\n",
       "         n_jobs=1, n_neighbors=100, p=2, radius=1.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "neigh_md = NearestNeighbors(n_neighbors=N,metric='mahalanobis', \n",
    "            metric_params= {'V': np.cov(np.transpose(training_images))})\n",
    "neigh_md.fit(gallery_images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_mahalanobi(k, N):\n",
    "    list_of_truths = []\n",
    "    start = time.time() #time tracking - start time of process\n",
    "    for index, query_index in enumerate(train_idxs['query_idx'].flatten().tolist()[:300]):\n",
    "        if (index % 50 == 0):\n",
    "            print(\"Index: \", index, \" & time taken: \", time.time() - start)\n",
    "        N_distances, N_indices = neigh_md.kneighbors(query_images[index].reshape(1, 2048), N)\n",
    "        topN_gallery_images = gallery_images[N_indices[0]]\n",
    "        forLoopStart = time.time()\n",
    "        topN_gallery_indices = train_idxs['gallery_idx'][N_indices[0]].flatten().tolist()\n",
    "        reduced_topN_indices, reduced_topN_labels = remove_indices(topN_gallery_indices, query_index)\n",
    "        if query_labels[index][0] in reduced_topN_labels[:k]:\n",
    "            list_of_truths.append(True)\n",
    "        else:\n",
    "            list_of_truths.append(False)\n",
    "    #over-all accuracy  \n",
    "    acc = sum(list_of_truths)/len(list_of_truths)\n",
    "    print(\"Avg Accuracy is: \", acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  8.082389831542969e-05\n",
      "Index:  50  & time taken:  1194.6821658611298\n",
      "Index:  100  & time taken:  2801.5326719284058\n",
      "Index:  150  & time taken:  4013.9011237621307\n",
      "Index:  200  & time taken:  5203.121114730835\n",
      "Index:  250  & time taken:  6391.557485818863\n",
      "Avg Accuracy is:  0.13666666666666666\n",
      "Index:  0  & time taken:  7.224082946777344e-05\n",
      "Index:  50  & time taken:  1187.0171492099762\n",
      "Index:  100  & time taken:  2373.66286110878\n",
      "Index:  150  & time taken:  3560.3639900684357\n",
      "Index:  200  & time taken:  4746.637277126312\n",
      "Index:  250  & time taken:  5932.8617470264435\n",
      "Avg Accuracy is:  0.2833333333333333\n",
      "Index:  0  & time taken:  9.083747863769531e-05\n",
      "Index:  50  & time taken:  1186.3573276996613\n",
      "Index:  100  & time taken:  2372.715752840042\n",
      "Index:  150  & time taken:  3559.498749732971\n",
      "Index:  200  & time taken:  4745.957722663879\n",
      "Index:  250  & time taken:  5932.393654823303\n",
      "Avg Accuracy is:  0.4533333333333333\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "k = [1, 10, 100]\n",
    "acc_k2 = []\n",
    "for k_val in k: \n",
    "    acc = get_accuracy_mahalanobi(k_val, N)\n",
    "    acc_k2.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13666666666666666, 0.2833333333333333, 0.4533333333333333]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='cosine',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=100, p=2, radius=1.0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "neigh_c = NearestNeighbors(n_neighbors=N,metric='cosine')\n",
    "neigh_c.fit(gallery_images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  & time taken:  7.700920104980469e-05\n",
      "Index:  50  & time taken:  4.227119207382202\n",
      "Index:  100  & time taken:  8.423174142837524\n",
      "Index:  150  & time taken:  12.630351066589355\n",
      "Index:  200  & time taken:  16.83267116546631\n",
      "Index:  250  & time taken:  21.04805612564087\n",
      "Avg Accuracy is:  0.49\n",
      "Index:  0  & time taken:  5.1975250244140625e-05\n",
      "Index:  50  & time taken:  4.1925787925720215\n",
      "Index:  100  & time taken:  8.413705825805664\n",
      "Index:  150  & time taken:  12.6040678024292\n",
      "Index:  200  & time taken:  16.792085886001587\n",
      "Index:  250  & time taken:  20.98653793334961\n",
      "Avg Accuracy is:  0.74\n",
      "Index:  0  & time taken:  5.3882598876953125e-05\n",
      "Index:  50  & time taken:  4.198216199874878\n",
      "Index:  100  & time taken:  8.467583894729614\n",
      "Index:  150  & time taken:  12.67863917350769\n",
      "Index:  200  & time taken:  16.884459972381592\n",
      "Index:  250  & time taken:  21.093831062316895\n",
      "Avg Accuracy is:  0.91\n"
     ]
    }
   ],
   "source": [
    "def get_acc_gen(k, N, index, query_index):\n",
    "    N_distances, N_indices = neigh_c.kneighbors(query_images[index].reshape(1, 2048), N)\n",
    "    topN_gallery_images = gallery_images[N_indices[0]]\n",
    "    forLoopStart = time.time()\n",
    "    topN_gallery_indices = train_idxs['gallery_idx'][N_indices[0]].flatten().tolist()\n",
    "    reduced_topN_indices, reduced_topN_labels = remove_indices(topN_gallery_indices, query_index)\n",
    "    return (query_labels[index][0] in reduced_topN_labels[:k])\n",
    "\n",
    "k = [1, 10, 100]\n",
    "acc_k3 = []\n",
    "for k_val in k:\n",
    "    list_of_truths = [] \n",
    "    start = time.time() #time tracking - start time of process\n",
    "    for index, query_index in enumerate(train_idxs['query_idx'].flatten().tolist()[:300]):\n",
    "        if (index % 50 == 0):\n",
    "            print(\"Index: \", index, \" & time taken: \", time.time() - start)\n",
    "        truth_false = get_acc_gen(k_val, N, index, query_index)\n",
    "        list_of_truths.append(truth_false)\n",
    "    acc = sum(list_of_truths)/len(list_of_truths)\n",
    "    print(\"Avg Accuracy is: \", acc)\n",
    "    acc_k3.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5328)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.linalg.inv(np.cov(np.transpose(reduced_training_images)))\n",
    "distance_matrix = cdist(reduced_query_images, reduced_gallery_images, 'mahalanobis', VI=S)\n",
    "distance_matrix.shape #rows represent query images, columns represent gallery images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the aggregate distance constraint? \n",
    "def loss_function():\n",
    "    y = np.zeros((1400, 5328),dtype=object) \n",
    "    for q_index, q_image in enumerate(reduced_query_images):\n",
    "        for g_index, g_image in enumerate(reduced_gallery_images):\n",
    "            if gallery_labels[g_index] == query_labels[q_index]: \n",
    "                y[q_index][g_index] = 1\n",
    "            else:\n",
    "                y[q_index][g_index] = 0\n",
    "    distances = []\n",
    "    for i in range(len(query_labels)):\n",
    "        agg_dist = np.dot(y[i, :],distance_matrix.T[:, i])\n",
    "        distances.append(agg_dist)\n",
    "    return sum(distances)\n",
    "\n",
    "#print(loss_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_y(X, Y, X_labels, Y_labels):\n",
    "    y = np.zeros((len(X[:, 0]), len(Y[:, 0])),dtype=object) \n",
    "    for t1_index, t1_image in enumerate(X):\n",
    "        for t2_index, t2_image in enumerate(Y):\n",
    "            if X_labels[t1_index] == Y_labels[t2_index]: \n",
    "                y[t1_index][t2_index] = 1\n",
    "            else:\n",
    "                y[t1_index][t2_index] = 0\n",
    "    return y\n",
    "\n",
    "def loss_function_training(A):\n",
    "    distance_matrix_training = cdist(reduced_training_images, reduced_training_images, 'mahalanobis', VI=A)   \n",
    "    agg_dist = sum([np.dot(y_train[i, :],distance_matrix_training.T[:, i]) for i in range(len(training_labels))])\n",
    "    return agg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "345445.9685922416\n"
     ]
    }
   ],
   "source": [
    "print(loss_function_training(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "y_train = calculate_y(reduced_training_images, reduced_training_images, training_labels, training_labels)\n",
    "#y_test = calculate_y(reduced_query_images, reduced_gallery_images, query_labels, gallery_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = np.linalg.inv(np.cov(np.transpose(reduced_training_images)))\n",
    "optimization_learning = minimize(loss_function_training, A0, method='CG', options={'maxiter': 1, 'disp' : b True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimization_learning.A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
